"""
WFP Macro Data Scraper
L·∫•y c√°c ch·ªâ s·ªë kinh t·∫ø vƒ© m√¥ (L·∫°m ph√°t, Gi√° c·∫£ th·ªã tr∆∞·ªùng, T·ª∑ gi√°) t·ª´ WFP th√¥ng qua HDX CKAN API
"""
import asyncio
import json
import os
import re
from datetime import datetime
from typing import List, Dict, Optional, Any
import aiohttp
import pandas as pd
from io import StringIO

# Constants
HDX_API_BASE = "https://data.humdata.org/api/3/action"
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(SCRIPT_DIR, "data")
OUTPUT_FILE = os.path.join(DATA_DIR, "wfp_macro_data.json")

# Ensure data directory exists
os.makedirs(DATA_DIR, exist_ok=True)

# Keywords ƒë·ªÉ t√¨m ki·∫øm c√°c ch·ªâ s·ªë Macro
# Format: (search_query, priority_keywords_in_title)
# priority_keywords_in_title: ∆Øu ti√™n dataset c√≥ t·ª´ kh√≥a n√†y trong title
MACRO_KEYWORDS = [
    ("market monitor", ["Global", "WFP Global"]),  # ∆Øu ti√™n Global Market Monitor
    ("food price", ["Global"]),  # ∆Øu ti√™n Global Food Prices
    ("economic explorer", []),
    ("inflation", []),
    ("exchange rate", []),
]


async def search_wfp_macro(keyword: str, priority_keywords: List[str] = None, session: aiohttp.ClientSession = None) -> Optional[Dict[str, Any]]:
    """
    T√¨m ki·∫øm dataset WFP macro data m·ªõi nh·∫•t theo keyword.
    
    Args:
        keyword: T·ª´ kh√≥a t√¨m ki·∫øm (v√≠ d·ª•: "market monitor", "food price")
        priority_keywords: Danh s√°ch t·ª´ kh√≥a ∆∞u ti√™n trong title (v√≠ d·ª•: ["Global"])
        session: aiohttp session ƒë·ªÉ th·ª±c hi·ªán request
    
    Returns:
        Metadata c·ªßa dataset m·ªõi nh·∫•t ho·∫∑c None n·∫øu kh√¥ng t√¨m th·∫•y
    """
    if priority_keywords is None:
        priority_keywords = []
    
    print(f"üîç Searching for WFP dataset: '{keyword}'...")
    if priority_keywords:
        print(f"   Priority: datasets with '{', '.join(priority_keywords)}' in title")
    
    url = f"{HDX_API_BASE}/package_search"
    params = {
        "q": keyword,  # Ch·ªâ t√¨m keyword, kh√¥ng c·∫ßn th√™m "organization:wfp" ·ªü ƒë√¢y
        "fq": "organization:wfp",  # Filter theo WFP organization
        "rows": 20,  # L·∫•y nhi·ªÅu k·∫øt qu·∫£ h∆°n ƒë·ªÉ c√≥ th·ªÉ filter theo priority
        "sort": "metadata_modified desc",  # S·∫Øp x·∫øp theo ng√†y c·∫≠p nh·∫≠t m·ªõi nh·∫•t
    }
    
    try:
        async with session.get(url, params=params) as response:
            if response.status != 200:
                print(f"   ‚ö†Ô∏è API returned status {response.status}")
                return None
            
            data = await response.json()
            
            if not data.get("success"):
                print(f"   ‚ö†Ô∏è API returned success=False: {data.get('error', {}).get('message', 'Unknown error')}")
                return None
            
            results = data.get("result", {}).get("results", [])
            
            if not results:
                print(f"   ‚ö†Ô∏è No datasets found for keyword: '{keyword}'")
                return None
            
            # N·∫øu c√≥ priority keywords, t√¨m dataset c√≥ ch·ª©a t·ª´ kh√≥a ƒë√≥ trong title
            selected_dataset = None
            if priority_keywords:
                for dataset in results:
                    title = dataset.get('title', '').upper()
                    if any(priority.upper() in title for priority in priority_keywords):
                        selected_dataset = dataset
                        print(f"   ‚úÖ Found priority dataset: '{dataset.get('title', 'N/A')}'")
                        break
            
            # N·∫øu kh√¥ng t√¨m th·∫•y priority, l·∫•y dataset m·ªõi nh·∫•t
            if selected_dataset is None:
                selected_dataset = results[0]
                print(f"   ‚úÖ Found dataset: '{selected_dataset.get('title', 'N/A')}'")
            
            print(f"   üìÖ Last modified: {selected_dataset.get('metadata_modified', 'N/A')}")
            
            return selected_dataset
    
    except Exception as e:
        print(f"   ‚ùå Error searching for '{keyword}': {e}")
        return None


def get_resource_url(dataset_json: Dict[str, Any], preferred_formats: List[str] = None) -> Optional[Dict[str, str]]:
    """
    T√¨m resource URL c·ªßa file d·ªØ li·ªáu s·∫°ch nh·∫•t t·ª´ dataset.
    
    Args:
        dataset_json: JSON metadata c·ªßa dataset t·ª´ API
        preferred_formats: Danh s√°ch format ∆∞u ti√™n (m·∫∑c ƒë·ªãnh: ['CSV', 'JSON'])
    
    Returns:
        Dict ch·ª©a url, format, name c·ªßa resource ho·∫∑c None n·∫øu kh√¥ng t√¨m th·∫•y
    """
    if preferred_formats is None:
        preferred_formats = ['CSV', 'JSON', 'XLSX', 'XLS']
    
    resources = dataset_json.get("resources", [])
    
    if not resources:
        print("   ‚ö†Ô∏è No resources found in dataset")
        return None
    
    print(f"   üì¶ Found {len(resources)} resources, filtering...")
    
    # L·ªçc resources theo format v√† lo·∫°i b·ªè metadata/readme files
    valid_resources = []
    
    for resource in resources:
        resource_format = resource.get("format", "").upper()
        resource_name = resource.get("name", "").lower()
        resource_url = resource.get("url", "")
        
        # B·ªè qua file metadata, readme, ho·∫∑c kh√¥ng c√≥ URL
        if not resource_url or any(skip in resource_name for skip in ["metadata", "readme", "guide"]):
            continue
        
        # ∆Øu ti√™n format CSV, JSON
        if resource_format in preferred_formats:
            priority = preferred_formats.index(resource_format) if resource_format in preferred_formats else 999
            valid_resources.append({
                "url": resource_url,
                "format": resource_format,
                "name": resource.get("name", "Unknown"),
                "description": resource.get("description", ""),
                "priority": priority,
                "size": resource.get("size", 0),
            })
    
    if not valid_resources:
        print("   ‚ö†Ô∏è No valid data resources found (CSV/JSON)")
        return None
    
    # S·∫Øp x·∫øp theo priority (CSV > JSON > XLSX > XLS) v√† size (file l·ªõn h∆°n th∆∞·ªùng ch·ª©a nhi·ªÅu data h∆°n)
    valid_resources.sort(key=lambda x: (x["priority"], -x["size"]))
    
    best_resource = valid_resources[0]
    print(f"   ‚úÖ Selected resource: {best_resource['name']} ({best_resource['format']})")
    print(f"   üîó URL: {best_resource['url'][:80]}...")
    
    return {
        "url": best_resource["url"],
        "format": best_resource["format"],
        "name": best_resource["name"],
    }


async def fetch_csv_data(url: str, session: aiohttp.ClientSession, max_rows: int = 1000) -> Optional[pd.DataFrame]:
    """
    T·∫£i v√† ƒë·ªçc CSV data t·ª´ URL.
    Ch·ªâ ƒë·ªçc m·ªôt ph·∫ßn file ƒë·ªÉ tr√°nh t·∫£i qu√° nhi·ªÅu d·ªØ li·ªáu.
    
    Args:
        url: URL c·ªßa file CSV
        session: aiohttp session
        max_rows: S·ªë d√≤ng t·ªëi ƒëa ƒë·ªÉ ƒë·ªçc (m·∫∑c ƒë·ªãnh 1000 d√≤ng ƒë·∫ßu)
    
    Returns:
        DataFrame ho·∫∑c None n·∫øu l·ªói
    """
    print(f"   üì• Downloading CSV data...")
    
    try:
        async with session.get(url) as response:
            if response.status != 200:
                print(f"   ‚ö†Ô∏è Failed to download: HTTP {response.status}")
                return None
            
            # ƒê·ªçc file theo chunks ƒë·ªÉ tr√°nh memory issue
            content = await response.text()
            
            # ƒê·ªçc CSV v·ªõi pandas
            df = pd.read_csv(StringIO(content), nrows=max_rows)
            
            print(f"   ‚úÖ Loaded {len(df)} rows, {len(df.columns)} columns")
            print(f"   üìä Columns: {', '.join(df.columns[:5].tolist())}...")
            
            return df
    
    except Exception as e:
        print(f"   ‚ùå Error fetching CSV: {e}")
        return None


async def extract_macro_indicators(df: pd.DataFrame, dataset_title: str) -> List[Dict[str, Any]]:
    """
    Extract c√°c ch·ªâ s·ªë macro t·ª´ DataFrame.
    
    Args:
        df: DataFrame ch·ª©a d·ªØ li·ªáu
        dataset_title: T√™n c·ªßa dataset
    
    Returns:
        List c√°c ch·ªâ s·ªë ƒë√£ extract
    """
    indicators = []
    
    # T√¨m c√°c c·ªôt quan tr·ªçng
    columns_lower = [col.lower() for col in df.columns]
    
    # T√¨m c·ªôt gi√° c·∫£
    price_cols = [col for col in df.columns if any(keyword in col.lower() for keyword in ['price', 'cost', 'value'])]
    
    # T√¨m c·ªôt l·∫°m ph√°t (bao g·ªìm c·∫£ change/trend columns)
    inflation_cols = [col for col in df.columns if any(keyword in col.lower() for keyword in ['inflation', 'change', 'trend', 'yoy'])]
    
    # T√¨m c·ªôt t·ª∑ gi√°
    exchange_cols = [col for col in df.columns if any(keyword in col.lower() for keyword in ['exchange', 'rate', 'usd', 'currency'])]
    
    # T√¨m c·ªôt qu·ªëc gia/khu v·ª±c
    country_cols = [col for col in df.columns if any(keyword in col.lower() for keyword in ['country', 'location', 'region', 'market'])]
    
    # T√¨m c·ªôt th·ªùi gian
    date_cols = [col for col in df.columns if any(keyword in col.lower() for keyword in ['date', 'time', 'month', 'year', 'period'])]
    
    print(f"   üîç Found columns:")
    print(f"      - Price: {price_cols[:3]}")
    print(f"      - Inflation: {inflation_cols[:3]}")
    print(f"      - Exchange Rate: {exchange_cols[:3]}")
    print(f"      - Country: {country_cols[:3]}")
    print(f"      - Date: {date_cols[:3]}")
    
    # Sort theo c·ªôt Date n·∫øu c√≥ ƒë·ªÉ l·∫•y d·ªØ li·ªáu m·ªõi nh·∫•t
    if len(df) > 0:
        # T√¨m c·ªôt date ƒë·ªÉ sort
        date_col = None
        for col in date_cols:
            if col in df.columns:
                try:
                    # Th·ª≠ convert sang datetime ƒë·ªÉ sort
                    df[col] = pd.to_datetime(df[col], errors='coerce')
                    date_col = col
                    break
                except:
                    continue
        
        # Sort theo date n·∫øu t√¨m th·∫•y
        if date_col:
            df_sorted = df.sort_values(by=date_col, na_position='last')
            latest_row = df_sorted.iloc[-1].to_dict()
        else:
            # N·∫øu kh√¥ng c√≥ date column, l·∫•y row cu·ªëi c√πng
            latest_row = df.iloc[-1].to_dict()
        
        indicator = {
            "dataset": dataset_title,
            "extracted_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "indicators": {}
        }
        
        # Extract gi√° c·∫£
        if price_cols:
            for col in price_cols[:3]:  # L·∫•y 3 c·ªôt ƒë·∫ßu
                value = latest_row.get(col)
                if pd.notna(value):
                    indicator["indicators"][f"price_{col.lower().replace(' ', '_')}"] = float(value) if isinstance(value, (int, float)) else str(value)
        
        # Extract l·∫°m ph√°t
        if inflation_cols:
            for col in inflation_cols[:2]:
                value = latest_row.get(col)
                if pd.notna(value):
                    indicator["indicators"][f"inflation_{col.lower().replace(' ', '_')}"] = float(value) if isinstance(value, (int, float)) else str(value)
        
        # Extract t·ª∑ gi√°
        if exchange_cols:
            for col in exchange_cols[:2]:
                value = latest_row.get(col)
                if pd.notna(value):
                    indicator["indicators"][f"exchange_rate_{col.lower().replace(' ', '_')}"] = float(value) if isinstance(value, (int, float)) else str(value)
        
        # Extract th√¥ng tin qu·ªëc gia/khu v·ª±c
        if country_cols:
            for col in country_cols[:2]:
                value = latest_row.get(col)
                if pd.notna(value):
                    indicator["indicators"][f"location_{col.lower().replace(' ', '_')}"] = str(value)
        
        # Extract th·ªùi gian
        if date_cols:
            for col in date_cols[:1]:
                value = latest_row.get(col)
                if pd.notna(value):
                    indicator["indicators"]["period"] = str(value)
        
        # Th√™m metadata v·ªÅ dataset
        indicator["metadata"] = {
            "total_rows": len(df),
            "columns": df.columns.tolist(),
            "price_columns": price_cols,
            "inflation_columns": inflation_cols,
            "exchange_rate_columns": exchange_cols,
        }
        
        indicators.append(indicator)
    
    return indicators


async def process_keyword(keyword_tuple: tuple, session: aiohttp.ClientSession) -> List[Dict[str, Any]]:
    """
    X·ª≠ l√Ω m·ªôt keyword: t√¨m dataset, l·∫•y resource, download v√† extract data.
    
    Args:
        keyword_tuple: Tuple (keyword, priority_keywords) ho·∫∑c string keyword
        session: aiohttp session
    
    Returns:
        List c√°c ch·ªâ s·ªë ƒë√£ extract
    """
    # Parse keyword tuple ho·∫∑c string
    if isinstance(keyword_tuple, tuple):
        keyword, priority_keywords = keyword_tuple
    else:
        keyword = keyword_tuple
        priority_keywords = []
    
    print(f"\n{'='*60}")
    print(f"Processing keyword: '{keyword}'")
    if priority_keywords:
        print(f"Priority keywords: {priority_keywords}")
    print(f"{'='*60}")
    
    # B∆∞·ªõc 1: T√¨m dataset
    dataset = await search_wfp_macro(keyword, priority_keywords, session)
    
    if not dataset:
        return []
    
    # B∆∞·ªõc 2: L·∫•y resource URL
    resource_info = get_resource_url(dataset)
    
    if not resource_info:
        return []
    
    # B∆∞·ªõc 3: Download v√† parse data
    if resource_info["format"] == "CSV":
        df = await fetch_csv_data(resource_info["url"], session, max_rows=1000)
        
        if df is not None and len(df) > 0:
            # B∆∞·ªõc 4: Extract indicators
            indicators = await extract_macro_indicators(df, dataset.get("title", keyword))
            
            # Th√™m th√¥ng tin v·ªÅ resource
            for indicator in indicators:
                indicator["resource_url"] = resource_info["url"]
                indicator["resource_name"] = resource_info["name"]
                indicator["resource_format"] = resource_info["format"]
            
            return indicators
    
    return []


def save_to_json(all_indicators: List[Dict[str, Any]]):
    """
    L∆∞u t·∫•t c·∫£ indicators v√†o file JSON.
    """
    output_data = {
        "last_updated": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "total_datasets": len(all_indicators),
        "indicators": all_indicators
    }
    
    try:
        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=4, ensure_ascii=False)
        print(f"\nüíæ Data saved to {OUTPUT_FILE}")
        print(f"üìä Total indicators extracted: {len(all_indicators)}")
    except Exception as e:
        print(f"\n‚ùå Error saving data: {e}")


async def main():
    """
    Main function ƒë·ªÉ crawl WFP macro data.
    """
    print("üöÄ Starting WFP Macro Data Scraper...")
    print(f"üìÅ Output directory: {DATA_DIR}")
    keyword_list = [kw[0] if isinstance(kw, tuple) else kw for kw in MACRO_KEYWORDS]
    print(f"üîç Keywords to search: {', '.join(keyword_list)}")
    print()
    
    all_indicators = []
    
    async with aiohttp.ClientSession() as session:
        # X·ª≠ l√Ω t·ª´ng keyword
        for keyword_config in MACRO_KEYWORDS:
            try:
                indicators = await process_keyword(keyword_config, session)
                all_indicators.extend(indicators)
                
                # Delay gi·ªØa c√°c request ƒë·ªÉ tr√°nh rate limit
                await asyncio.sleep(2)
            
            except Exception as e:
                keyword_str = keyword_config[0] if isinstance(keyword_config, tuple) else keyword_config
                print(f"‚ùå Error processing keyword '{keyword_str}': {e}")
                continue
    
    # L∆∞u k·∫øt qu·∫£
    if all_indicators:
        save_to_json(all_indicators)
    else:
        print("\n‚ö†Ô∏è No indicators extracted. Please check the keywords or API availability.")
    
    print("\nüèÅ Scraper finished.")


if __name__ == "__main__":
    asyncio.run(main())

